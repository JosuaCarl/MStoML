{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import sys\n",
    "sys.path.append( '../FIA' )\n",
    "sys.path.append( '../ML' )\n",
    "\n",
    "from FIA import *\n",
    "from ML4com import *\n",
    "\n",
    "import keras\n",
    "from keras.layers import Input, Dense, Lambda\n",
    "from keras.layers import BatchNormalization, Dropout, LeakyReLU\n",
    "from keras.models import Model\n",
    "from keras.losses import mse\n",
    "from keras.optimizers.legacy import Adam\n",
    "from keras import backend as K\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "import keras_tuner\n",
    "from keras_tuner.tuners import Hyperband, BayesianOptimization\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading names:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 72/72 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading experiments:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 68/68 [00:00<00:00, 100.46it/s]\n"
     ]
    }
   ],
   "source": [
    "info_dir = \"../../data/comm8_self\"\n",
    "data_dir = \"../../runs/FIA/comm8/oms\"\n",
    "run_dir = \"../../runs/ML/try\"\n",
    "\n",
    "info_dir = os.path.normpath(os.path.join(os.getcwd(), info_dir))\n",
    "data_dir = os.path.normpath(os.path.join(os.getcwd(), data_dir))\n",
    "run_dir = os.path.normpath(os.path.join(os.getcwd(), run_dir))\n",
    "\n",
    "strains = pd.read_csv(os.path.join(info_dir, \"strains.tsv\"), sep=\"\\t\")\n",
    "comm8 = pd.read_csv(os.path.join(info_dir, \"comm8.tsv\"), sep=\"\\t\")\n",
    "\n",
    "fia_df = load_fia_df(data_dir, file_ending=\".mzML\", separator=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# binned_dfs = bin_df_stepwise_batch(fia_df, binning_var=\"mz\", binned_var=\"inty\", statistic=\"sum\", start=50.0, stop=1700.0, step=0.002)\n",
    "# binned_dfs.to_csv(os.path.join(run_dir, \"data_matrix.tsv\"), sep=\"\\t\")\n",
    "binned_dfs = pd.read_csv(os.path.join(run_dir, \"data_matrix_oms.tsv\"), sep=\"\\t\", index_col=\"mz\", engine=\"pyarrow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MaxAbsScaler()\n",
    "binned_dfs[:] =  scaler.fit_transform(binned_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(825000, 68)\n",
      "(68, 8)\n",
      "(8, 1)\n"
     ]
    }
   ],
   "source": [
    "print(binned_dfs.shape)\n",
    "print(comm8.shape)\n",
    "print(strains.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data & model configuration\n",
    "batch_size = 32\n",
    "no_epochs = 1000\n",
    "latent_dim = 18\n",
    "\n",
    "original_dim = binned_dfs.shape[0]\n",
    "input_shape = (original_dim,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_builder(hp):\n",
    "    # # =================\n",
    "    # # Encoder\n",
    "    # # =================\n",
    "\n",
    "    # Definition\n",
    "    i       = Input(shape=input_shape, name='encoder_input')\n",
    "    \n",
    "    x       = Dense(hp.Int('encoder_units', min_value=30, max_value=220, step=10))(i)\n",
    "    x       = LeakyReLU()(x)\n",
    "    \n",
    "    mu      = Dense(latent_dim, name='latent_mu')(x)\n",
    "    sigma   = Dense(latent_dim, name='latent_sigma')(x)\n",
    "\n",
    "    # Define sampling with reparameterization trick\n",
    "    def sample_z(args):\n",
    "        mu, sigma = args\n",
    "        batch     = K.shape(mu)[0]\n",
    "        dim       = K.int_shape(mu)[1]\n",
    "        eps       = K.random_normal(shape=(batch, dim))\n",
    "        return mu + K.exp(sigma / 2) * eps\n",
    "\n",
    "    # Use reparameterization trick to ....??\n",
    "    z       = Lambda(sample_z, output_shape=(latent_dim, ), name='z')([mu, sigma])\n",
    "\n",
    "    # Instantiate encoder\n",
    "    encoder = Model(i, [mu, sigma, z], name='encoder')\n",
    "    \n",
    "    # =================\n",
    "    # Decoder\n",
    "    # =================\n",
    "\n",
    "    # Definition\n",
    "    d_i   = Input(shape=(latent_dim, ), name='decoder_input')\n",
    "    \n",
    "    x     = Dense(hp.Int('decoder_units', min_value=20, max_value=220, step=10))(d_i)\n",
    "    x     = LeakyReLU()(x)\n",
    "        \n",
    "    o     = Dense(original_dim)(x)\n",
    "\n",
    "    # Instantiate decoder\n",
    "    decoder = Model(d_i, o, name='decoder')\n",
    "    \n",
    "    # =================\n",
    "    # VAE as a whole\n",
    "    # =================\n",
    "\n",
    "    # Define loss\n",
    "    def kl_reconstruction_loss(true, pred):\n",
    "      # Reconstruction loss\n",
    "        reconstruction_loss = mse(true, pred)\n",
    "        reconstruction_loss *= original_dim\n",
    "\n",
    "        # KL divergence loss\n",
    "        kl_loss = 1 + sigma - K.square(mu) - K.exp(sigma)\n",
    "        kl_loss = K.sum(kl_loss, axis=-1)\n",
    "        kl_loss *= -0.5\n",
    "        \n",
    "        # weight KL divergence loss here\n",
    "        kl_loss *= hp.Float('kl_beta', min_value=1e-3, max_value=1e1, sampling='LOG', default=1e-2)\n",
    "\n",
    "        return K.mean(reconstruction_loss + kl_loss)\n",
    "\n",
    "    # Instantiate VAE\n",
    "    vae_outputs = decoder(encoder(i)[2])\n",
    "    vae         = Model(i, vae_outputs, name='vae')\n",
    "\n",
    "    # Define optimizer\n",
    "    optimizer = Adam(hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='LOG', default=1e-3))\n",
    "\n",
    "    # Compile VAE\n",
    "    vae.compile(optimizer=optimizer, loss=kl_reconstruction_loss, metrics = ['mse'], experimental_run_tf_function=False)\n",
    "    \n",
    "    return vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set tuner parameters\n",
    "tuner = Hyperband(\n",
    "    model_builder,\n",
    "    objective='mse',\n",
    "    factor=2,\n",
    "    max_epochs=200,\n",
    "    directory='hyperband_optimization',\n",
    "    project_name='mtvae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search space summary\n",
      "Default search space size: 4\n",
      "encoder_units (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 30, 'max_value': 220, 'step': 10, 'sampling': 'linear'}\n",
      "decoder_units (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 20, 'max_value': 220, 'step': 10, 'sampling': 'linear'}\n",
      "learning_rate (Float)\n",
      "{'default': 0.001, 'conditions': [], 'min_value': 0.0001, 'max_value': 0.01, 'step': None, 'sampling': 'log'}\n",
      "kl_beta (Float)\n",
      "{'default': 0.01, 'conditions': [], 'min_value': 0.001, 'max_value': 10.0, 'step': None, 'sampling': 'log'}\n"
     ]
    }
   ],
   "source": [
    "tuner.search_space_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = binned_dfs.transpose()\n",
    "ys = comm8\n",
    "kf = KFold(n_splits = 5, shuffle=True)     # stratified: skf = StratifiedKFold(n_splits=5, random_state=42, shuffle=True)\n",
    "\n",
    "training_data, test_data, training_labels, test_labels = train_test_split(X, ys, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5 Complete [00h 00m 05s]\n",
      "mse: 4.165573773207143e-05\n",
      "\n",
      "Best mse So Far: 1.733126373437699e-05\n",
      "Total elapsed time: 00h 02m 15s\n",
      "\n",
      "Search: Running Trial #6\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "170               |100               |encoder_units\n",
      "40                |210               |decoder_units\n",
      "0.00028832        |0.00027583        |learning_rate\n",
      "0.85359           |1.665             |kl_beta\n",
      "2                 |2                 |tuner/epochs\n",
      "0                 |0                 |tuner/initial_epoch\n",
      "7                 |7                 |tuner/bracket\n",
      "0                 |0                 |tuner/round\n",
      "\n",
      "Train on 54 samples, validate on 14 samples\n",
      "Epoch 1/2\n",
      "54/54 [==============================] - ETA: 0s - loss: 24.2796 - mse: 2.9424e-05"
     ]
    }
   ],
   "source": [
    "tuner.search(training_data, training_data, validation_data = (test_data, test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Return best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.results_summary(num_trials = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.get_best_models()[0].summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the optimal hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials = 1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the optimal hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials = 1)[0]\n",
    "\n",
    "print(best_hps.get('encoder_units'))\n",
    "print(best_hps.get('decoder_units'))\n",
    "print(best_hps.get('learning_rate'))\n",
    "print(best_hps.get('kl_beta'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append( '../../../mtVAE' )\n",
    "from models import *\n",
    "from metric_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data & model configuration\n",
    "input_dim = original_dim\n",
    "intermediate_dim = 200\n",
    "latent_dim = 18\n",
    "\n",
    "kl_beta = 1e-2\n",
    "learning_rate = 1e-3\n",
    "\n",
    "batch_size = 32\n",
    "n_epochs = 1000\n",
    "\n",
    "save_folder = '../../runs/VAE'\n",
    "\n",
    "\n",
    "# instantiate model\n",
    "mtmodel = mtVAE(input_dim, intermediate_dim, latent_dim, kl_beta, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "mtmodel.train(training_data, test_data, n_epochs, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "mtmodel.save_model(save_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtmodel.reconstruct(training_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gemml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
